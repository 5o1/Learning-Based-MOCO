{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms as tf\n",
    "from models.mynn import functional as myf\n",
    "from models.mynn.loss import ssim\n",
    "from train import train\n",
    "import typing\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from skimage.data import shepp_logan_phantom\n",
    "from skimage.transform import resize\n",
    "import models as mymodels\n",
    "\n",
    "class Phantom(Dataset):\n",
    "\n",
    "    def __init__(self, image_size : tuple = (256,256), transforms: typing.Callable = lambda x:x, debug = False):\n",
    "        super().__init__()\n",
    "        self.transforms = transforms\n",
    "        self.image_size = image_size\n",
    "\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        sample = shepp_logan_phantom()\n",
    "        sample = resize(sample, self.image_size, anti_aliasing=True)\n",
    "        sample = torch.tensor(sample).float().view(1, *self.image_size)\n",
    "        # sample = (sample - sample.mean()) / sample.std()\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            sample = self.transforms(sample)\n",
    "        return sample\n",
    "    \n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "\n",
    "class Motion(nn.Module):\n",
    "    def __init__(self, image_size = (320, 320),\n",
    "                 motion_ratio = [1, 1, 1, 1, 1], rot = 15, shift = (0.05, 0.05), scale = (0.01, 0.01), shear = (0.01, 0.01),\n",
    "                 num_spokes_full = 500, num_spokes_partial = 20, num_pts_readout = 320, oversampling_factor = 2,\n",
    "                 dtype = torch.complex64, device = torch.device('cuda')):\n",
    "        super().__init__()\n",
    "        self.image_size = image_size\n",
    "        self.rot = rot\n",
    "        self.shift = shift\n",
    "        self.scale = scale\n",
    "        self.shear = shear\n",
    "        self.num_spokes_full = num_spokes_full\n",
    "        self.num_spokes_partial = num_spokes_partial\n",
    "        self.num_pts_readout = num_pts_readout\n",
    "        self.oversampling_factor = oversampling_factor\n",
    "        self.dtype = dtype\n",
    "        self.device = device\n",
    "\n",
    "        self.float_dtype = torch.float32 if dtype == torch.complex64 else torch.float64\n",
    "\n",
    "        self.num_motion_states = len(motion_ratio)\n",
    "        self.motion_partition = [0]\n",
    "        for ratio in motion_ratio:\n",
    "            self.motion_partition.append(self.motion_partition[-1] + ratio / sum(motion_ratio))\n",
    "        \n",
    "        self.random_motion = tf.RandomAffine(degrees=rot, translate=shift, scale=scale, shear=shear, fill=0).to(device)\n",
    "\n",
    "\n",
    "    def move(self, image):\n",
    "        \"\"\" input: batch, channel, height, width\n",
    "            output: motion_state, batch, channel, height, width\n",
    "        \"\"\"\n",
    "        image = myf.complex_to_real(image) # batch, channel * 2, height, width\n",
    "        resl = torch.zeros((self.num_motion_states+1), *image.shape, dtype=image.dtype, device=image.device) # motion_state batch, channel, height, width\n",
    "        resl[0] = image\n",
    "        for i in range(self.num_motion_states):\n",
    "            resl[i+1] = self.random_motion(image)\n",
    "        resl = myf.real_to_complex(resl) # motion_state, batch, channel, height, width\n",
    "        return resl\n",
    "    \n",
    "\n",
    "    def ft(self, image):\n",
    "        res = myf.itok(image)\n",
    "        return res\n",
    "    \n",
    "    def ift(self, kspace):\n",
    "        res = myf.ktoi(kspace)\n",
    "        return res\n",
    "\n",
    "    def forward(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        image = self.move(image) # motion_state, batch, channel, height, width\n",
    "        kspace = self.ft(image) # motion_state, batch, channel, phase, readout\n",
    "\n",
    "        indices = torch.randperm(kspace.shape[-2])\n",
    "        kspace_mixed = torch.zeros_like(kspace[0]) # batch, channel, phase, readout\n",
    "        for state in range(self.num_motion_states):\n",
    "            kspace_mixed[:, :, indices[int(self.motion_partition[state] * indices.shape[0]):int(self.motion_partition[state+1] * indices.shape[0])], :] \\\n",
    "                = kspace[state][:, :, indices[int(self.motion_partition[state] * indices.shape[0]):int(self.motion_partition[state+1] * indices.shape[0])] , :]\n",
    "            \n",
    "        # in cartesian space, acs lines are forced to be motion-free, acs is 8% of the phase\n",
    "        center_phases = kspace_mixed.shape[-2] // 2\n",
    "        kspace_mixed[:,:,center_phases - int(kspace_mixed.shape[-2] * 0.04):center_phases + int(kspace_mixed.shape[-2] * 0.04), :] = kspace[0][:,:,center_phases - int(kspace_mixed.shape[-2] * 0.04):center_phases + int(kspace_mixed.shape[-2] * 0.04), :]\n",
    "\n",
    "        image = self.ift(kspace_mixed)\n",
    "        return image, kspace_mixed\n",
    "\n",
    "\n",
    "class DataAugmentation(nn.Module):\n",
    "    \"\"\"Computationally Intensive Transformations\"\"\"\n",
    "    def __init__(self, motion_simulator: nn.Module):\n",
    "        super().__init__()\n",
    "        self.motion_simulator = motion_simulator\n",
    "\n",
    "    def mean_std_norm_complex(self, data: torch.Tensor) -> torch.Tensor:\n",
    "        real = data.real\n",
    "        imag = data.imag\n",
    "        real = (real - real.mean()) / real.std()\n",
    "        imag = (imag - imag.mean()) / imag.std()\n",
    "        return real + 1j * imag\n",
    "\n",
    "    def forward(self, data):\n",
    "        image = data + 0j\n",
    "        kspace = myf.itok(image)\n",
    "        image_after, kspace_after = self.motion_simulator(image)\n",
    "        return {\"kspace_before\":kspace, \"kspace_after\":kspace_after, \"image_before\":image, \"image_after\":image_after}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import math\n",
    "from typing import List\n",
    "from einops import rearrange, repeat, pack, unpack\n",
    "\n",
    "class SinusoidalPositionEncoding1d(nn.Module):\n",
    "    \"\"\"Sinusoidal Position Encoding for 1-dimensions\"\"\"\n",
    "    def __init__(self, d_model, pos_scale=1e2):\n",
    "        super(SinusoidalPositionEncoding1d, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.pos_scale = pos_scale\n",
    "        self.conv1d = nn.Conv1d(1, d_model, 1)\n",
    "\n",
    "        self.register_buffer('_div_term', torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)))\n",
    "\n",
    "    def forward(self, tensor = None, position = None):\n",
    "        \"\"\"tensor:[batch_size, seq_len, d_model]  \n",
    "        position:[batch_size, seq_len, 1]  \n",
    "        tensor is None: return positional encoding  \n",
    "        position is None: return [0-1]*scale positional encoding for tensor  \n",
    "        \"\"\"\n",
    "        assert tensor is not None or position is not None, \"Either tensor or position must be provided\"\n",
    "\n",
    "        if position is None:\n",
    "            position = torch.arange(tensor.size(1), device=tensor.device).unsqueeze(0).unsqueeze(-1).float()\n",
    "        position = (position - position.min()) / (position.max() - position.min())\n",
    "        position = position * self.pos_scale\n",
    "\n",
    "        assert len(position.size()) == 3, \"position must be [batch_size, seq_len, 1]\"\n",
    "\n",
    "        pe = torch.zeros(position.size(0), position.size(1), self.d_model, device=position.device)\n",
    "        pe[:, :, 0::2] = torch.sin(position * self._div_term)\n",
    "        pe[:, :, 1::2] = torch.cos(position * self._div_term)\n",
    "\n",
    "        if tensor is None:\n",
    "            return pe        \n",
    "        return tensor + pe\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SinusoidalPositionEncodingmd(nn.Module):\n",
    "    \"\"\"Multi-dimensional Sinusoidal Position Encoding\"\"\"\n",
    "    def __init__(self, d_model, pos_scale: float | List[float] = 1e2, n_dims=2):\n",
    "        super(SinusoidalPositionEncodingmd, self).__init__()\n",
    "        self.n_dims = n_dims\n",
    "        self.d_model = d_model\n",
    "        self.register_buffer('pos_scale', torch.tensor(pos_scale if isinstance(pos_scale, list) else [pos_scale] * n_dims))\n",
    "\n",
    "        assert d_model % n_dims == 0, \"d_model must be divisible by n_dims\"\n",
    "        self.register_buffer('_div_term', torch.exp(torch.arange(0, self.d_model // self.n_dims, 2).float() * -(math.log(10000.0) / self.d_model // self.n_dims)))\n",
    "\n",
    "    \n",
    "    def forward(self, tensor, position = None):\n",
    "        \"\"\"tensor:[batch_size, seq_len, d_model]  \n",
    "        position:[batch_size, seq_len, n_dims]  \n",
    "        tensor is None: return positional encoding   \n",
    "        position is None: return [0-1]*scale positional encoding for tensor   \n",
    "        \"\"\"\n",
    "        assert tensor is not None or position is not None, \"Either tensor or position must be provided\"\n",
    "\n",
    "        if position is None:\n",
    "            position = []\n",
    "            for i in range(self.n_dims):\n",
    "                pos = torch.arange(tensor.size(1), device=tensor.device).unsqueeze(0).unsqueeze(-1).float()\n",
    "                position.append(pos)\n",
    "            position = torch.cat(position, dim=-1)\n",
    "        position = (position - position.min()) / (position.max() - position.min())\n",
    "        position = position * self.pos_scale\n",
    "\n",
    "        assert len(position.size()) == 3, \"position must be [batch_size, seq_len, n_dims]\"\n",
    "\n",
    "        pe = torch.zeros(position.size(0), position.size(1), self.d_model, device=position.device)\n",
    "        for i in range(self.n_dims): # [aaabbbccc]\n",
    "            pe[:, :, i * self.d_model // self.n_dims:(i+1) * self.d_model // self.n_dims:2] = torch.sin(position[:, :, i:i+1] * self._div_term)\n",
    "            pe[:, :, i * self.d_model // self.n_dims + 1:(i+1) * self.d_model // self.n_dims:2] = torch.cos(position[:, :, i:i+1] * self._div_term)\n",
    "\n",
    "        if tensor is None:\n",
    "            return pe\n",
    "        return tensor + pe\n",
    "\n",
    "\n",
    "class LearnablePositionEncoding(nn.Module):\n",
    "    \"\"\"Learnable Position Encoding\"\"\"\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super(LearnablePositionEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "        self._pe = nn.Parameter(torch.randn(max_len, d_model))\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        return tensor + self._pe[:tensor.size(0), :]\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from einops import rearrange, repeat, pack, unpack\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout),\n",
    "                FeedForward(dim, mlp_dim, dropout = dropout)\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "    \n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, seq_len, patch_size, num_classes, dim, depth, heads, mlp_dim, channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
    "        super().__init__()\n",
    "        assert (seq_len % patch_size) == 0\n",
    "\n",
    "        num_patches = seq_len // patch_size\n",
    "        patch_dim = channels * patch_size\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (n p) -> b n (p c)', p = patch_size),\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "            nn.LayerNorm(dim),\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, series):\n",
    "        x = self.to_patch_embedding(series)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, 'd -> b d', b = b)\n",
    "\n",
    "        x, ps = pack([cls_tokens, x], 'b * d')\n",
    "\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        cls_tokens, _ = unpack(x, ps, 'b * d')\n",
    "\n",
    "        return self.mlp_head(cls_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models.mynn as mynn\n",
    "from models.mynn import functional as myf\n",
    "\n",
    "from torchkbnufft import KbNufft, KbNufftAdjoint, calc_tensor_spmatrix, calc_density_compensation_function, ToepNufft\n",
    "\n",
    "class MAE_Kradial(nn.Module):\n",
    "    def __init__(self, \n",
    "                 image_size = (320, 320),\n",
    "                 mask_ratio = 0.5,\n",
    "                 pts_readout = 640,\n",
    "                 in_channels = 16, out_channels = 16,\n",
    "                 d_model_encoder = 1024, d_model_decoder = 1024,\n",
    "                 nlayers_encoder = 4, nlayers_decoder = 4,\n",
    "                 nheads_encoder = 8, nheads_decoder = 8,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.image_size = image_size\n",
    "        self.d_model_encoder = d_model_encoder\n",
    "        self.d_model_decoder = d_model_decoder\n",
    "        self.nlayer_encoder = nlayers_encoder\n",
    "        self.nlayer_decoder = nlayers_decoder\n",
    "        self.nheads_encoder = nheads_encoder\n",
    "        self.nheads_decoder = nheads_decoder\n",
    "        self.mask_ratio = mask_ratio\n",
    "        self.pts_readout = pts_readout\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        ### nufft object\n",
    "        self._nufft_obj = KbNufft(im_size=image_size, device='cuda')\n",
    "        self._inufft_obj = KbNufftAdjoint(im_size=image_size, device='cuda')\n",
    "\n",
    "        ### Encoder\n",
    "        self.input_embed = nn.Sequential(\n",
    "            mynn.IFFTn(dim = -1),\n",
    "            mynn.Complex2Real(),\n",
    "            Rearrange('batch channel phase readout -> batch phase (readout channel)'),\n",
    "            nn.LayerNorm(pts_readout * in_channels * 2), \n",
    "            nn.Linear(pts_readout * in_channels * 2, d_model_encoder, bias=True),\n",
    "            nn.LayerNorm(d_model_encoder)\n",
    "        )\n",
    "\n",
    "        self.norm_encoder = nn.LayerNorm(d_model_encoder)\n",
    "\n",
    "        self.pe_encoder = SinusoidalPositionEncoding1d(d_model = d_model_encoder, pos_scale=100)\n",
    "\n",
    "        self.encoder = Transformer(dim = d_model_encoder, depth = nlayers_encoder, heads = nheads_encoder, mlp_dim = 2048, dim_head = d_model_encoder // nheads_encoder)\n",
    "\n",
    "\n",
    "        ### Decoder\n",
    "        self.decoder_embed = nn.Linear(d_model_encoder, d_model_decoder, bias=True)\n",
    "\n",
    "        self.pe_decoder = SinusoidalPositionEncoding1d(d_model = d_model_decoder, pos_scale=100)\n",
    "\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, d_model_decoder))\n",
    "\n",
    "        self.decoder = Transformer(dim = d_model_decoder, depth = nlayers_decoder, heads = nheads_decoder, mlp_dim = 2048, dim_head = d_model_decoder // nheads_decoder)\n",
    "\n",
    "        self.norm_decoder = nn.LayerNorm(d_model_decoder)\n",
    "\n",
    "        self.output_embed = nn.Sequential(\n",
    "            nn.Linear(d_model_decoder, pts_readout * out_channels * 2, bias=True),\n",
    "            Rearrange('batch phase (readout channel) -> batch channel phase readout', channel = out_channels * 2, readout = pts_readout),\n",
    "            mynn.Real2Complex(),\n",
    "            mynn.FFTn(dim = -1),\n",
    "        )\n",
    "\n",
    "\n",
    "    def nufft(self, image, traj):\n",
    "        device = image.device\n",
    "        image = image.to(torch.device(\"cuda\"))\n",
    "        traj = traj.to(torch.device(\"cuda\"))\n",
    "\n",
    "        traj_shape = traj.shape\n",
    "        traj = traj / 160 * torch.pi # bart normalization to torchkbnufft normalization\n",
    "        traj = torch.view_as_real(traj)\n",
    "        traj = rearrange(traj, 'phase readout pos -> pos (readout phase)')\n",
    "\n",
    "        res = self._nufft_obj(image, traj)\n",
    "        res = rearrange(res, '... (readout phase) ->... phase readout', phase=traj_shape[0], readout=traj_shape[1])\n",
    "\n",
    "        res = res.to(device)\n",
    "        return res\n",
    "    \n",
    "    def inufft(self, kspace, traj):\n",
    "        device = kspace.device\n",
    "        kspace = kspace.to(torch.device(\"cuda\"))\n",
    "        traj = traj.to(torch.device(\"cuda\"))\n",
    "\n",
    "        traj_shape = traj.shape\n",
    "        traj = traj / 160 * torch.pi # bart normalization to torchkbnufft normalization\n",
    "        traj = torch.view_as_real(traj)\n",
    "        traj = rearrange(traj, 'phase readout pos -> pos (readout phase)')\n",
    "        kspace = rearrange(kspace, '... phase readout ->... (readout phase)')\n",
    "\n",
    "        interp_mats = calc_tensor_spmatrix(traj,im_size=self.image_size, table_oversamp=2)\n",
    "        dcomp = calc_density_compensation_function(ktraj=traj, im_size=self.image_size)\n",
    "\n",
    "        res = self._inufft_obj(kspace * dcomp, traj, interp_mats)\n",
    "        res = res.to(device)\n",
    "        return res\n",
    "\n",
    "    def mean_std_norm_complex(self, data: torch.Tensor, dim = [-1, -2, -3]):\n",
    "        real = data.real\n",
    "        imag = data.imag\n",
    "        real = (real - real.mean(dim=dim, keepdim = True)) / real.std(dim=dim, keepdim = True)\n",
    "        imag = (imag - imag.mean(dim=dim, keepdim = True)) / imag.std(dim=dim, keepdim = True)\n",
    "        return real + 1j * imag\n",
    "\n",
    "\n",
    "    def random_masking(self, x, mask_ratio):\n",
    "        \"\"\"\n",
    "        https://github.com/facebookresearch/mae/tree/main\n",
    "\n",
    "        Perform per-sample random masking by per-sample shuffling.\n",
    "        Per-sample shuffling is done by argsort random noise.\n",
    "        x: [N, L, D], sequence\n",
    "        \"\"\"\n",
    "        N, L, D = x.shape  # batch, length, dim\n",
    "        len_keep = int(L * (1 - mask_ratio))\n",
    "        \n",
    "        noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]\n",
    "        \n",
    "        # sort noise for each sample\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
    "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "\n",
    "        # keep the first subset\n",
    "        ids_keep = ids_shuffle[:, :len_keep]\n",
    "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
    "\n",
    "        # generate the binary mask: 0 is keep, 1 is remove\n",
    "        mask = torch.ones([N, L], device=x.device)\n",
    "        mask[:, :len_keep] = 0\n",
    "        # unshuffle to get the binary mask\n",
    "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "\n",
    "        return x_masked, mask, ids_restore\n",
    "    \n",
    "    def forward_encoder_train(self, x, pos, mask_ratio):\n",
    "        # x: [batch, phase, readout * channel * 2]\n",
    "        # pos: [batch, phase, 1]\n",
    "\n",
    "        ### Encoder\n",
    "        # in training mode, the shape of kdata is matched with the shape of ktraj\n",
    "        # kdata should be masked as same as ktraj\n",
    "        x = self.pe_encoder(x, pos)\n",
    "        x, mask, ids_restore = self.random_masking(x, mask_ratio)\n",
    "\n",
    "        # cls token\n",
    "        pass\n",
    "\n",
    "        # encode\n",
    "        \n",
    "        x = self.encoder(x)\n",
    "        x = self.norm_encoder(x)\n",
    "\n",
    "        return x, mask, ids_restore\n",
    "    \n",
    "    def forward_decoder_train(self, x, pos, ids_restore):\n",
    "        # x: [batch, phase, readout * channel * 2]\n",
    "        # pos: [batch, phase, 1]\n",
    "\n",
    "        ### Decoder\n",
    "        # embed tokens\n",
    "        x = self.decoder_embed(x)\n",
    "\n",
    "        # append mask tokens to sequence\n",
    "        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] - x.shape[1], 1)\n",
    "        x = torch.cat([x, mask_tokens], dim=1)\n",
    "        x = torch.gather(x, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
    "\n",
    "        # pe\n",
    "        x = self.pe_decoder(x, pos)\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        x = self.decoder(x)\n",
    "        x = self.norm_decoder(x)\n",
    "\n",
    "        # predictor projection\n",
    "        x = self.output_embed(x)\n",
    "\n",
    "        # remove cls token\n",
    "        pass\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward_train(self, kdata, ktraj):\n",
    "        # kdata: [batch, channel, phase, readout]\n",
    "        # ktraj: [batch, phase, readout]\n",
    "        kdata = self.input_embed(kdata) # [batch, phase, readout * channel]\n",
    "        ktraj_angle = torch.angle(ktraj).mean(dim = -1).unsqueeze(-1) # [batch, angle]\n",
    "\n",
    "        ktraj_angle = ktraj_angle[:,:kdata.shape[1]]\n",
    "        # encode\n",
    "        encoder_memory, mask, ids_restore = self.forward_encoder_train(kdata, ktraj_angle, self.mask_ratio)\n",
    "        \n",
    "        # decode\n",
    "        kdata_pred = self.forward_decoder_train(encoder_memory, ktraj_angle, ids_restore)\n",
    "\n",
    "        return kdata_pred, ktraj[:,:kdata_pred.shape[2]], mask\n",
    "\n",
    "\n",
    "    def forward_encoder_eval(self, x, pos):\n",
    "        # x: [batch, phase, readout * channel * 2]\n",
    "        # pos: [batch, phase, 1]\n",
    "\n",
    "        ### Encoder\n",
    "        # in evaluation mode, the shape of kdata is not matched with the shape of ktraj\n",
    "        # kdata should be masked as same as ktraj\n",
    "        x = self.pe_encoder(x, pos)\n",
    "\n",
    "        # cls token\n",
    "        pass\n",
    "\n",
    "        # encode\n",
    "        x = self.encoder(x)\n",
    "        x = self.norm_encoder(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def forward_decoder_eval(self, x, pos):\n",
    "        # x: [batch, phase, readout * channel * 2]\n",
    "        # pos: [batch, phase, 1]\n",
    "\n",
    "        ### Decoder\n",
    "        # embed tokens\n",
    "        x = self.decoder_embed(x)\n",
    "\n",
    "        # append mask tokens to sequence\n",
    "        mask_tokens = self.mask_token.repeat(x.shape[0], 1, 1)\n",
    "        x = torch.cat([x, mask_tokens], dim=1)\n",
    "\n",
    "        # pe\n",
    "        x = self.pe_decoder(x, pos)\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        x = self.decoder(x)\n",
    "        x = self.norm_decoder(x)\n",
    "\n",
    "        # predictor projection\n",
    "        x = self.output_embed(x)\n",
    "\n",
    "        # remove cls token\n",
    "        pass\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def forward_eval(self, kdata, ktraj):\n",
    "        # kdata: [batch, channel, phase, readout]\n",
    "        # ktraj: [batch, phase, readout]\n",
    "        kdata = self.input_embed(kdata)\n",
    "        ktraj_angle = torch.angle(ktraj).mean(dim = -1).unsqueeze(-1)\n",
    "\n",
    "        ktraj_angle = ktraj_angle[:,:kdata.shape[1]]\n",
    "        # encode\n",
    "        encoder_memory = self.forward_encoder_eval(kdata, ktraj_angle)\n",
    "\n",
    "        # decode\n",
    "        kdata_pred = self.forward_decoder_eval(encoder_memory, ktraj_angle)\n",
    "\n",
    "        return kdata_pred, ktraj[:,:kdata_pred.shape[2]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
